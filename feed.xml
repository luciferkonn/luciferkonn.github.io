<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://luciferkonn.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://luciferkonn.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-12-31T16:52:07+00:00</updated><id>https://luciferkonn.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">HyperTransformer Model Generation for Supervised and Semi-supervised Few-show Learning</title><link href="https://luciferkonn.github.io/blog/2022/HyperT/" rel="alternate" type="text/html" title="HyperTransformer Model Generation for Supervised and Semi-supervised Few-show Learning" /><published>2022-04-14T00:00:00+00:00</published><updated>2022-04-14T00:00:00+00:00</updated><id>https://luciferkonn.github.io/blog/2022/HyperT</id><content type="html" xml:base="https://luciferkonn.github.io/blog/2022/HyperT/"><![CDATA[<p>üìñ Arxiv: <a href="https://arxiv.org/abs/2201.04182">2201.04182</a></p>

<h2 id="motivation">Motivation</h2>

<ol>
  <li>One approach of few-shot learning is <strong>metric-based</strong></li>
</ol>

<blockquote>
  <p>üí° The main idea is to use the transformer model that given a few-shot task episode generates an entire inference model by producing all model weights in a single pass.</p>
</blockquote>

<h2 id="contribution">Contribution</h2>

<ol>
  <li>
    <p><strong>Small CNN Architectures:</strong> this method is effective than training a universal task-independent embedding.</p>
  </li>
  <li>
    <p><strong>Large CNN Architectures:</strong></p>

    <blockquote>
      <p>We develop a novel replay buffer consistent with the architecture and training protocol of ODT</p>
    </blockquote>
  </li>
</ol>

<h2 id="methodology">Methodology</h2>

<ol>
  <li>This paper combines DT with SAC, which adopt a maximum-entropy idea to encourage the exploration in fine-tuning.</li>
  <li>Minor changes:
    <ol>
      <li>Change replay buffer from saving transitions to trajectories.</li>
      <li>Utilizing HER to improve the sample-efficiency in sparse rewards settings.</li>
      <li>Sampling strategy.</li>
    </ol>
  </li>
</ol>

<!-- <aside>
‚ùì what is cross conditional entropy?

</aside> -->

<h2 id="references">References</h2>

<ol>
  <li>Levine, Sergey. ‚ÄúReinforcement Learning and Control as Probabilistic Inference: Tutorial and Review.‚Äù¬†<em>ArXiv</em> abs/1805.00909 (2018): n. pag.</li>
  <li>Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight Experience Replay. In Advances in Neural Information Processing Systems, 2017.</li>
</ol>]]></content><author><name>Andrey Zhmoginov</name></author><category term="hypernetworks" /><category term="transformer" /><category term="paper-reading" /><summary type="html"><![CDATA[üìñ Arxiv: 2201.04182]]></summary></entry><entry><title type="html">Online Decision Transformer</title><link href="https://luciferkonn.github.io/blog/2022/ODT/" rel="alternate" type="text/html" title="Online Decision Transformer" /><published>2022-04-10T00:00:00+00:00</published><updated>2022-04-10T00:00:00+00:00</updated><id>https://luciferkonn.github.io/blog/2022/ODT</id><content type="html" xml:base="https://luciferkonn.github.io/blog/2022/ODT/"><![CDATA[<p>üìñ Arxiv: <a href="https://arxiv.org/abs/2202.05607">2202.05607</a></p>

<h2 id="motivation">Motivation</h2>

<!-- <aside> -->
<p>üí° An online algorithms, which access to offline data can have zero or even negative effect on the online performance.</p>

<!-- </aside> -->

<h2 id="contribution">Contribution</h2>

<ol>
  <li>
    <p><strong>Exploration:</strong></p>

    <blockquote>
      <p>We shift from deterministic to stochastic policies for defining exploration objectives during the online phase.</p>
    </blockquote>
  </li>
  <li>
    <p><strong>A New Replay Buffer:</strong></p>

    <blockquote>
      <p>We develop a novel replay buffer consistent with the architecture and training protocol of ODT</p>
    </blockquote>
  </li>
</ol>

<h2 id="methodology">Methodology</h2>

<ol>
  <li>This paper combines DT with SAC, which adopt a maximum-entropy idea to encourage the exploration in fine-tuning.</li>
  <li>Minor changes:
    <ol>
      <li>Change replay buffer from saving transitions to trajectories.</li>
      <li>Utilizing HER to improve the sample-efficiency in sparse rewards settings.</li>
      <li>Sampling strategy.</li>
    </ol>
  </li>
</ol>

<!-- <aside>
‚ùì what is cross conditional entropy?

</aside> -->

<h2 id="references">References</h2>

<ol>
  <li>Levine, Sergey. ‚ÄúReinforcement Learning and Control as Probabilistic Inference: Tutorial and Review.‚Äù¬†<em>ArXiv</em> abs/1805.00909 (2018): n. pag.</li>
  <li>Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight Experience Replay. In Advances in Neural Information Processing Systems, 2017.</li>
</ol>]]></content><author><name>Qingqing Zheng</name></author><category term="reinforcement-learning" /><category term="decision-transformer" /><category term="transformer" /><category term="paper-reading" /><summary type="html"><![CDATA[combine decision transformer with SAC]]></summary></entry></feed>